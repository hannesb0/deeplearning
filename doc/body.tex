% !TeX root = ../main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%																				%%
%% File name: 		body.tex													%%
%% Project name:	Applications in Deep Learning								%%
%% Type of work:	Advanced Seminar											%%
%% Author:			Hannes Bohnengel											%%
%% Mentor:			Debayan Roy													%%
%% Date:			13 July 2017												%%
%% University:		Technical University of Munich								%%
%% Comments:		Created in texstudio with tab width = 4						%%
%%																				%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conventional Speech Synthesis}
\label{sec:speech}

\subsection{Motivation and Approaches}
\label{subsec:convenspeech}

Speech synthesis has emerged over the last ten years due to a vast contribution by the global community of researchers and the increasing computational power for data processing. Its quality and naturalness has increased steadily and different approaches have been developed so far~\cite{suendermann:challenges}. The typical applications like navigation systems in cars or telephone-based dialogue systems are nowadays widely established. But also as reading aid for visually impaired people~\cite{readspeaker:tts} or as in the case of the famous scientist Stephen Hawking, who has been using a synthesized voice to communicate since 1997~\cite{hawking:speech}, speech synthesis has proven to be very useful. Another very interesting application of speech synthesis is shown in~\cite{dhavala:communication}. Here, the author proposed to introduce synthetic speech as means of communication between pilots, since there have been many accidents due to misunderstandings at radio-based communication. %Finally, the interaction between man and machines or robots~\cite{hill:manmachine}

According to~\cite{hinterleitner:quality}, speech synthesis can be divided into three types: Canned speech, \ac{CTS} and Text-to-Speech~(\acsu{TTS}). Canned speech more or less is the playback of prerecorded spoken sentences or words, both with none or very little adjustments. Typical examples are the announcements on train stations. Because of the high effort of recording everything (almost) exactly as it is played back, this approach is limited to a few simple applications. With the second type, \ac{CTS}, the waveform is generated out of a linguistic description without any information of the respective text. In this way, no natural language processing is required, but nevertheless \ac{CTS} has not made any important impact yet. The last and most promising type is \ac{TTS}. A \ac{TTS} system consists of a \ac{NLP} part, where the text is analyzed and the word and sentence structure and accents are extracted. In the next step, these accents are used to generate the prosody of the given text like duration, intensity and pitch. Then, the created phonetic representations with prosody information are stringed together to a continuous stream of signal parameters. The last task, the speech generation, uses this stream to generate the respective waveform. This function block can be implemented in different ways. In~\cite{hinterleitner:quality}, three general approaches are named as follows: Parametric Speech Synthesis (formant-based synthesis), Concatenative Speech Synthesis (unit-selection synthesis) and \acf{SPSS} (\acf{HMM}-based synthesis). The methods in brackets are the respective implementations, which are most commonly used. 

The formant-based synthesis is the oldest approach. To generate a voice waveform, an excitation signal is fed into multiple formant filters, which describe the characteristics of the human vocal tract. The output of the filters then forms the voice waveform. This technique is the only one which does not need any recorded speech, but instead generates the synthesized voice only by modeling the human vocal tract. On the one hand, the quality of the generated voice is the lowest in comparison to the other techniques, on the other hand, formant-based synthesizers have the smallest footprint and their voice characteristics can easily been modified by just changing their filter parameters~\cite{hinterleitner:quality}.
With the development of concatenative speech synthesizer, the quality of the generated speech improved tremendously. Very similar to \ac{CTS} prerecorded speech is used as reference. Basically, the recorded speech is divided into units and these units are then stringed together to form the new speech signal according to a given text. Hereby, the chosen size of the units determines both the footprint size and the voice quality. With larger units a higher voice quality can be achieved, but this also results in a much larger database. The challenge in unit-selection is to ensure that the transitions between the units are as natural as possible~\cite{hinterleitner:quality}. In Section~\ref{subsec:hmmspeech}, the \ac{HMM}-based synthesis, a specific instance of \ac{SPSS} is described~in~detail.

\subsection{\ac{HMM}-based Speech Synthesis}
\label{subsec:hmmspeech}

In this section, the \ac{HMM}-based speech synthesis which is an instance of \ac{SPSS} and the most recent approach will be described further. Additionally, both advantages and drawbacks compared to unit-selection synthesis will be highlighted. Therefore, the work of Black~\textit{et~al.}~\cite{black:statistical} will be taken as reference since this work is widely accepted and commonly cited when dealing with this topic.

The quality of unit-selection synthesis directly depends on the quality of the prerecorded speech. But even with a database of excellent quality, sporadic errors can still not be avoided totally. If a specific phonetic or prosodic part of a generated sentence is not well represented in the database the output quality of this sentence suffers immensely. To try to avoid this, a huge effort in specifically designing the database for the required application can be performed, but still there is no guarantee that such bad joins will not happen. In addition, the fact that in unit-selection no or only very little adaptions of the voice characteristics are possible without an enormous increase of the database size, the ambition towards seamless speech synthesis leads towards the \ac{HMM}-based approach. Here, a statistic representation of some sets of speech segments is used to generate arbitrary synthetic speech. In Figure~\ref{fig:hmm}, the structure of a typical \ac{HMM}-based synthesizer is shown. The whole system can be divided into two parts, the training and the synthesis part. The connection of these two parts is a set of context-dependent \acp{HMM}. 

\begin{figure}[h]
	\includegraphics[width=1\columnwidth]{hmm-system.pdf}
	\caption{Function blocks of \ac{HMM}-based synthesis~\cite{black:statistical}}
	\label{fig:hmm}
\end{figure}

\noindent In the training part spectrum and excitation parameters % (DETAILS???)
of the recorded speech are used to generate acoustic models represented by the \acp{HMM}. Therefore, phonetic, linguistic and prosodic parameters are considered. In comparison to a unit-selection system, the large speech database is only needed in the training part. In the synthesis part, first the text which is to be synthesized is transformed to a sequence of parameters containing information about the context. % (what context???)
According to this sequence, the respective \acp{HMM} are concatenated in order to form an utterance \ac{HMM}. Then, after determining the state durations of the \acp{HMM}, a sequence of coefficients is created, which finally is used to construct the speech waveform using a specific filter (\textit{e.g.} a \ac{MLSA}~filter). The main disadvantage of this approach compared to unit-selection synthesis is the quality of the synthesized speech. Three factors are accountable for the lack of quality: the vocoder, the modeling accuracy, and an effect called over-smoothing. However, there are some essential advantages, which make the \ac{HMM}-based approach a competitive alternative to unit-selection synthesis. First, the voice characteristics can be modified without much effort. Thus, the implementation of different languages and the realization of different speaking styles with emotional emphasis is possible. Second, these aspects require a much smaller database than in unit-selection synthesis, since only a statistic representation of speech segments rather than raw speech data is stored. Finally, the quality of the synthesized speech of a \ac{HMM}-based system is much more robust and free of sporadic flaws. In Table~\ref{tab:speechgeneration}, the techniques discussed so far are compared regarding the most prominent advantages and drawbacks.


\begin{table}[h]
	\caption{Comparison of speech generation methods~\cite{hinterleitner:quality, black:statistical}}
	\label{tab:speechgeneration}
	\vspace{-0.75em}
	\resizebox{0.9\columnwidth}{!}{
		\begin{minipage}{\columnwidth}
			\begin{tabularx}{1\columnwidth}{cYY}
				\toprule
				\textbf{Technique} & \textbf{Advantages} & \textbf{Drawbacks}\\
				\midrule
				Formant-based & Very small \ \ \ \ \ footprint & Very artificial and metallic voice\\
				& & \\[-0.5em]
				Unit-selection & Very high voice quality possible & Large database required\\
				& & \\[-0.5em]
				\ac{HMM}-based & Adjustable voice and small footprint & Voice sounds \ \ muffled\\
				\bottomrule
			\end{tabularx}
		\end{minipage}}
		\vspace{-0.75em}
\end{table}
	
%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------
% SPSS with Deep Learning Models
%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------

\section{\ac{SPSS} with Deep Learning Models}
\label{sec:deepspeech}

Although the usage of \ac{SPSS} has brought many advantages over unit-selection synthesis as shown in the previous section, the generated voice is still not as natural as desired. Therefore, deep learning models recently have been used to further improve \ac{SPSS}. Since \acfp{DNN} have proven to be very effective in speech recognition, they have found their way into speech synthesis,~too~\cite{hashimoto:effect}. %With the use of \acp{DNN} it is possible to represent higher dimensional and correlated features in an efficient way as well as compactly modeling complex mapping functions. Exactly these properties can be used in \ac{SPSS}, where different features representing the context (linguistic, prosody, etc.) have to be considered for the acoustic modeling~\cite{hashimoto:effect}.
In the beginning of this section, one specific approach how to introduce deep learning models into an \ac{HMM}-based speech synthesis system is explained further by using~\cite{zen:deepstatistical} as reference. Then, some alternative techniques of enhancing the quality of speech synthesis through the deployment of deep learning models are outlined, as shown in the work of Hashimoto~\textit{et~al.}~\cite{hashimoto:effect}.

\subsection{One Specific Approach for Improvement}
\label{subsec:deepspss}

In the previous section, it is mentioned that the quality issue of \ac{HMM}-based speech synthesis is caused by three aspects: the vocoder, the accuracy of acoustic models and the over-smoothing effect. Zen \textit{et~al.}~\cite{zen:deepstatistical} suggest a specific approach to eliminate one of these causes, in particular the accuracy of acoustic models by allocating this task to~a~\ac{DNN}. 

In conventional \ac{HMM}-based systems the mapping between context features (phonetic and linguistic properties) and speech parameters is done by decision tree based context clustering. Thereby, the context-dependent \acp{HMM} are assigned to different clusters depending on the combination of contexts using binary decision trees. Each cluster is characterized by a specific set of speech parameters. In this way, it is possible to estimate all \acp{HMM} in a robust way with a typically sized training database. However, decision trees soon reach their limits when handling complex contexts. Only by increasing the size and in this way decreasing the efficiency of the decision tree, more complicated contexts (\textit{e.g.} XOR) can be dealt with. In addition to that, decision trees require partitioned input data with each partition having a different set of parameters. In this way, with less data per region overfitting is likely to happen, which then results in a lack of quality. These downsides can be avoided by using a \ac{DNN} instead of multiple decision~trees. Nevertheless, this also introduces two disadvantages: The first one arises in terms of computational power. Both, in the training and in the prediction stage, decision trees require much less operations (total amount and level of complexity) than \acp{DNN}. The second one has to do with the decision process in its basic form. With decision trees a binary question has to be answered, whilst a \ac{DNN} consists of weighted neurons, which use non-linear activation functions (\textit{e.g.} sigmoid, tanh, ReLU~\cite{chung:activation}) to determine their state. As consequence, interpretable rules are far easier to produce with decision trees than with \acp{DNN}.
In Figure~\ref{fig:dnnspeech}, the structure of a \ac{DNN}-based speech synthesis system is shown. First, a sequence of input features is generated after analyzing the input text. These parameters contain numeric values like the number of words in a sentence or the duration of a phoneme as well as binary answers to questions like ''is the current phoneme \textit{aa}?''. Then, this parameter sequence is fed into the \ac{DNN} where a mapping to output features is deployed by using forward propagation. The \ac{DNN} has to be trained before with pairs of input and output features from a database. In the following steps, the speech parameters are extracted from the statistics of the output features and the voice waveform in turn generated from the speech parameters. This is done in the same way as in the \ac{HMM}-based system. For this system the function blocks of text analysis, parameter generation, and waveform synthesis can be reused from an \ac{HMM}-based system. Only the mapping from input features (\textit{e.g.} linguistic contexts) to output features (spectral and excitation parameters) is implemented in a different way.

\begin{figure}[h]
	\includegraphics[width=0.9\columnwidth]{dnn-speech-small.pdf}
	\caption{Speech synthesis with usage of a \ac{DNN}~\cite{zen:deepstatistical}}
	\label{fig:dnnspeech}
\end{figure}

\noindent To compare the output of the above described framework with that of an \ac{HMM}-based, Zen~\textit{et~al.} conducted some experiments with each an \ac{HMM}-based and a \ac{DNN}-based speech synthesis system in~\cite{zen:deepstatistical}. Therefore, they used the same speech data in US English which includes about 33000 utterances for both systems. The \ac{HMM}-based system used 2554 questions for the decision tree-based context clustering. To influence the size of the decision trees, the scaling factor $\alpha$ is used, where a high value results in a small decision tree and $\alpha = 1$ denotes a typical \ac{HMM}-based system. 342 binary features (\textit{e.g.} phonemes identities) and 25 numerical features (\textit{e.g.} number of syllables in a word) represent the input features of the \ac{DNN}-based system. The sigmoid function is used as activation function, since the authors experienced superior performance of this type in previous tests. In total, one network with different number of layers (1, 2, 3, 4, or 5) and units per layer (256, 1024, or 2048) are used.

The objective evaluation showed that the \ac{DNN}-based system achieved better performance in voiced/unvoiced classification and aperiodicity prediction, regardless of the number of layers or the units per layer. At the Mel-cepstral distortion only \acp{DNN} with three or more layers outperformed the \ac{HMM}-based system. As subjective evaluation, 173 test sentences each synthesized with the \ac{DNN}- and the \ac{HMM}-based systems were played back to a number of listeners, who then choose which system they preferred. If no difference is perceived, the option ''neutral'' can be selected. In this test, the same number of parameters for both systems are used. For the structure of the \ac{DNN}-based approach, four layers with a different number of units per layer were applied. In Table~\ref{tab:subeval}, the outcome of the subjective evaluation is shown. Distinctly, the speech samples generated with the \ac{DNN}-based systems were preferred, regardless of the amount of units per layer. The listeners described them as less muffled.

\begin{table}[h]
	\caption{Subjective scores of speech samples~\cite{zen:deepstatistical}}
	\vspace{-0.75em}
	\label{tab:subeval}
	\begin{tabularx}{0.8\columnwidth}{cYc}
		\toprule
		\textbf{\ac{HMM}} & \textbf{\ac{DNN}} & \textbf{Neutral}\\
		($\alpha$) & (layers $\times$ units) & \\
		\midrule
		15.8~\%\enspace(16) & 38.5~\%\enspace(4 $\times$ 256) & 45.7~\%\\[0.5em]
		16.1~\%\enspace(4) & 27.2~\%\enspace(4 $\times$ 512) & 56.8~\%\\[0.5em]
		12.7~\%\enspace(1) & 36.6~\%\enspace(4 $\times$ 1024) & 50.7~\%\\
		\bottomrule
	\end{tabularx}
	%\vspace{-0.25em}
\end{table}

\noindent In conclusion, it can be stated that a \ac{DNN}-based approach to implement the acoustic model of a speech synthesis system is a reasonable alternative to the conventional decision tree-based strategy. The improved performance for predicting spectral and excitation parameters and the more natural sounding voice both show the potential of the \ac{DNN}-based approach for speech synthesis. However, the higher computational costs both at training and prediction stage due to more complex arithmetic operations in the \ac{DNN}-based approach indicate where future work should be focused on.

\subsection{Other Ways for Improvement}
\label{subsec:deepeffect}

The previous section focused on the impact a \ac{DNN} has, by representing the acoustic model of a speech synthesis system. However, deep learning models can be deployed in other parts of \ac{SPSS} as well. In~\cite{hashimoto:effect}, different ways on how to implement a \ac{DNN} into a speech synthesis system are investigated. Therefore, different parts of an \ac{HMM}-based speech synthesis system are once modeled with a \ac{DNN} and once with the conventional technique. Then, the results are compared in an objective and a subjective fashion.

Hashimoto~\textit{et~al.} concentrated on two core components of a speech synthesis system, the acoustic models and the speech generation part. In Figure~\ref{fig:generalspeech}, you can see the simplified structure of a speech synthesis system. In the first block, the text is analyzed and the contextual features are extracted (A). These are then converted to static and dynamic acoustic features (B) by the acoustic models. The parameter generation block then uses these acoustic features to create speech parameters (C). In the last step, the speech waveform is synthesized.

\begin{figure}[h]
	\includegraphics[width=0.8\columnwidth]{speech-general.pdf}
	\caption{Simplified structure of speech synthesis system~\cite{hashimoto:effect}}
	\label{fig:generalspeech}
\end{figure}

\noindent The two gray-colored blocks in Figure~\ref{fig:generalspeech} are subject of two experiments conducted in~\cite{hashimoto:effect}. The conventional approach for the acoustic models is the use of decision tree clustered \acp{HMM}, whereas the parameter generation usually is implemented by a \ac{MLPG} algorithm. As seen in Section~\ref{subsec:deepspss}, the acoustic models can be represented by a \ac{DNN}. Hashimoto \textit{et~al.} also included this approach in their experiments but furthermore used a \ac{DNN} for the speech parameter generation task. In Table~\ref{tab:experiments}, the systems resulting of the different combinations of the deployment of either conventional or \ac{DNN}-based approach are shown. The systems~I~-~IV are part of experiment~1 and all systems except system~IV are part of experiment~2.

\begin{table}[h]
	\caption{Different systems within the experiments~\cite{hashimoto:effect}}
	\vspace{-0.75em}
	\label{tab:experiments}
	\centering
	\resizebox{1\columnwidth}{!}{
		\begin{minipage}{\columnwidth}
			\begin{tabularx}{1\linewidth}{>{\hsize5.5em\arraybackslash}X>{\centering\hsize2.2em\arraybackslash}X>{\centering\hsize2.2em\arraybackslash}X>{\centering\hsize2.2em\arraybackslash}X>{\centering\hsize2em\arraybackslash}X>{\centering\hsize2.7em\arraybackslash}X>{\centering\hsize2.6em\arraybackslash}X}
				\toprule
				\textbf{System Nr.}& \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI}\\
				\midrule
				Experiments & 1\,\&\,2 & 1\,\&\,2 & 1\,\&\,2 & 1 & 2 & 2\\
				&&&&&&\\[-0.7em]
				Acoustic \ \ \ \ \ \ models & \ac{HMM} & \ac{HMM} & \ac{DNN} & \ac{DNN} & \ac{HMM} & \ac{HMM}\\
				&&&&&&\\[-0.7em]
				Speech \ \ \ \ \ \ \ \ \ \ \ \ generation & \ac{MLPG} & \ac{DNN} & \ac{MLPG} & \ac{DNN} & \ac{DNN}~+ \ac{MLPG} & \ac{MLPG} +~\ac{DNN}\\
				\bottomrule
			\end{tabularx}
		\end{minipage}}
	\vspace{-1.25em}
\end{table}

\noindent In experiment~1, all \acp{DNN} are equipped with three hidden layers and different number of units per layers (256, 512, or 1024), while the size of the decision trees was controlled similar as in~\cite{zen:deepstatistical}. Only the Mel-cepstral distortion was used as objective evaluation. Here, the systems with a \ac{DNN} (system~II~-~IV) all outperformed system~I in a similar way, as soon as they had 512 or more units per layer. For the subjective evaluation, 20 sentences were played back to eight listeners who had to assign a naturalness score between 1 and 5 (with 5 being the most natural). After applying the mean opinion score test method, system~III was most preferred with 3.53 followed by system~IV with 3.17 and system~I with 3.08. The less natural sounding voice according to the listeners was produced by system~II. From these results, it can be concluded that the speech generation part implemented with \ac{MLPG} (system~I \& III) produces a more natural output than those with a \ac{DNN} (system~II \& IV).

Due to these outcomes, experiment~2 introduces two new test cases, system~V and system~VI. For the parameter generation task a combination of \ac{MLPG} and \ac{DNN} is used, for which the respective input and output features were adapted. Again, once an objective and a subjective evaluation like in experiment~1 were conducted to measure the performance. The objective evaluation basically showed the same trend as in experiment~1, as soon as 512 or more units per layer were used, the Mel-cepstral distortion was smaller than with the conventional approach (system~I). In the subjective evaluation, the two new systems (V~\&~VI) both were preferred over system~I, with system~VI being the most preferred system of this experiment. System~III again was better and system~II again was evaluated less natural than system~I.

As conclusion, we see that the experiments conducted in~\cite{hashimoto:effect} confirmed the results of~\cite{zen:deepstatistical} that the decision tree-clustered \acp{HMM} can be replaced by a \ac{DNN} to improve both the prediction performance as well as the naturalness of the generated speech. Beyond that Hashimoto~\textit{et~al.} demonstrated that it is worthwhile to go one step further and deploy an additional \ac{DNN} as post-filtering block into the parameter generation task of the speech synthesis system to further improve the above stated results.

%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------
% Speech Synthesis on Embedded Devices
%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------

\section{Speech Synthesis on Mobile Devices}
\label{sec:embeddedspeech}

In this section, two approaches to optimize speech synthesis for mobile devices are presented. In the first, some optimization steps on a conventional \ac{HMM}-based system are suggested~\cite{toth:optimizing}, whereas the second approach introduces deep learning models to get rid of the dependence on Internet-based services~\cite{boros:robust}.

\subsection{Motivation and Challenges}
\label{subsec:motembedded}

During the last 10 years, advances in technology have led to a tremendous spread of mobile devices, especially smartphones. According to~\cite{statista:smartphones}, there are 2.1~billion smartphone users worldwide in 2016 which is forecasted to increase to almost 3~billion in 2020. In~\cite{statista:smartphones}, it also states that only in 2016 about 1.5~billion new smartphones were sold. From these numbers, we can derive that already today and with increasing significance in the near future, smartphones constitute an essential part of our daily life. Especially on smartphones where the visual output is restricted to a rather small screen, usually not bigger than 5 inch in diagonal, speech enabled interaction can improve the user experience in a significant way. In~\cite{toth:optimizing}, three application scenarios how speech interaction can assist are pointed out. Firstly, speech interaction can be used as an extension to existing communication channels. Secondly, if other outputs are restricted (\textit{e.g.} while driving), speech can be deployed as main output. Thirdly, speech can help visually impaired or blind people to interact with a system at all (\textit{e.g.}~screen~readers).

However, when implementing a speech synthesis application on a mobile device, the increased computational power and storage capacity on recent smartphones cannot be used as with desktop devices. One aspect is the power consumption. An application on a smartphone should use as less processing power as possible to avoid shortening the battery life unnecessarily~\cite{toth:optimizing}. Another issue is the restricted amount of main memory (16~-~32\,MB RAM per app), which forces each application to be as resource-saving as possible. While accomplishing these demands, the output speech still needs to be processed in real-time to ensure a good user experience~\cite{boros:robust}.

\subsection{Optimized \ac{HMM}-based Synthesis}
\label{subsec:hmmembedded}

One approach to optimize \ac{HMM}-based speech synthesis for mobile devices can be seen in~\cite{toth:optimizing}. Here, T\'oth~\textit{et~al.} conduct a study to optimize an \ac{HMM}-based speech synthesis system in terms of computational power, playback latency and footprint size while ensuring a reasonable quality of the synthesized speech. In the following, this approach is explained further.

As already pointed out in Section~\ref{subsec:convenspeech}, \ac{HMM}-based speech synthesis offers the best trade-off between speech quality and footprint size. This surely makes it the most appropriate approach to be deployed in mobile systems. T\'oth~\textit{et~al.} indicate that there already have been several approaches to optimize \ac{HMM}-based speech synthesis. However, they claim to be the first who emphasize the optimization steps on resource constrained platforms to achieve a reduction in computational costs, while still fulfilling real-time responsiveness. To measure the processing time of the speech synthesis procedure, T\'oth~\textit{et~al.} divide the \ac{TTS}-process in three steps: The loading of the \ac{HMM} database into main memory (1), the speech parameter generation (2), and the waveform synthesis (3). A~17~seconds sequence is chosen as test sentence, although it is unlikely that such a long utterance has to be synthesized often in real life (since in practice segments with one~to~five seconds are synthesized). That way, it can be ensured that shorter sequences are synthesized as desired. The following four approaches to optimize \ac{HMM}-based \ac{TTS} are suggested by T\'oth~\textit{et~al.}:

\begin{enumerate}[label=\Alph*)]		% label=\roman* OR label=\alph* OR label=\arabic*
	\parskip0.25em
	\bfseries
	\item Adjusting vocoder parameters
	\item Reducing the size of decision trees
	\item Introducing streaming synthesis
	\item Applying source code optimizations
\end{enumerate}

\noindent In the first approach, the conventional \ac{MLSA} filter used for speech parameter generation is replaced by a \ac{LSP} filter. In this way, the performance in step (2) can be enhanced. Different orders of the \ac{LSP} filter are tested to compare speech subjective quality, with an order of 18 being the conventional setup and 14, 12, and 10 being the optimization approaches. The second approach dealt with the size of the decision trees. While lowering the number of nodes of a decision tree reduces the quality of synthesized speech, it also decreases the memory footprint and the computational costs. Therefore, three different test settings were defined (see Table~\ref{tab:settings}).

\begin{table}[h]
	\caption{Test settings of optimization approach B)~\cite{toth:optimizing}}
	\vspace{-0.75em}
	\label{tab:settings}
	\resizebox{1\columnwidth}{!}{
	\begin{minipage}{\columnwidth}
		\centering
		\begin{tabularx}{0.75\columnwidth}{lYY}
			\toprule
			\textbf{} & \textbf{Nodes in \ \ \ decision trees} & \textbf{Footprint \ \ \ size}\\
			\midrule
			Original & 6983 & 666\,KB\\[0.5em]
			Setting 1 & 4762 & 463\,KB\\[0.5em]
			Setting 2 & 2743 & 214\,KB\\[0.5em]
			Setting 3 & 1273 & 140\,KB\\
			\bottomrule
		\end{tabularx}
	\end{minipage}}
	\vspace{-0.5em}
\end{table}

General speech synthesis frameworks usually do not include the audio playback functionality to ensure platform independence. Implementing this part allows streaming synthesis. In this way, one already synthesized segment is played back while the next segment is generated. While long segments cause an undesired latency, too short segments result in uncontinuous audio playback. The optimal segment size is determined at run-time to achieve a compromise between low as possible latency and low as possible \break computational effort. Low-resource devices differ from high-end devices not only in the amount of available memory and the computing performance, but also in the chip architecture. Memory management, fixed- versus floating-point calculation and conditional call management are points to be aware of. By implementing these issues in an architecture-optimized way, the processing load and time can be decreased. The above described optimizations were tested on three different smartphones with increasing CPU speed and main memory, starting with the ''weakest'': an Apple Iphone 3G, a Samsung Galaxy Spica (GT-i5700) and a HTC Desire (A8181). Therefore, the above described optimization steps were tested one by one, since they do not affect each other in a noteworthy manner.

For the first approach, a 12th~order \ac{LSP} filter was chosen as the optimal setting in terms of latency and subjective speech quality. Concerning the size of the decision trees, setting~1 clearly proved to be the optimal setting, since the time to load the \ac{HMM} database~(1) was decreased to half as before without notable impaired speech quality. The streaming synthesis approach tremendously decreased the duration of step (1), with an improvement from the order of seconds to 8~ms without affecting the speech quality at all. Concerning the source code optimizations, almost no optimizations have been conducted. Only some conditional calls were removed, which resulted in a insignificant improvement of latency. In total, the computation time could be decreased by 65\,\% while the speech~quality was impaired only negligibly.

\subsection{Deep Learning-based Synthesis}
\label{subsec:deepembedded}

The usual approach for current \acfp{VPA} (or generally \ac{HCI} interfaces) to use servers for processing input data and generating dedicated output actions, results in a total dependency on existing Internet access. In~\cite{boros:robust}, Boros~\textit{et~al.} are indicating three major disadvantages of this strategy: The availability of network access is not always granted (\textit{e.g.} bad coverage, additional charges abroad), the issue of data security, and possible delays due to bad network connection causing bad user experience. The authors are therefore suggesting the use of deep learning models to port an existing \ac{TTS} system (based on Romanian language) to a mobile device. Thereby, the main goal is to reduce the total memory footprint without loosing to much accuracy compared to the original system implementation. A \ac{TTS} system consists of a text processing front-end and a digital signal processing back-end. The suggestions made in~\cite{boros:robust} solely focus on the front-end. In this scope a deep learning model is introduced in each of the following text processing tasks:

\begin{enumerate}[label=\arabic*.]		% label=\roman* OR label=\alph* OR label=\arabic*
	\parskip0.25em
	\bfseries
	\item Syllabification (SYL)
	\item Phonetic transcription (PT)
	\item Part-of-speech tagging (POT)
	\item Lexical stress prediction (LSP)
\end{enumerate}

\noindent All experiments are conducted with a \ac{DNN} with two or three hidden layers respectively. Each hidden layer consists of an auto encoder. In the following, the experiments are explained further and the respective results are summarized in Table~\ref{tab:deepresults}. As reference system, the \ac{MIRA} is used~\cite{boros:mira}.

In the first task, the syllabification, each word is decomposed into syllables (phonological units). This contributes to the later conducted prosody generation. By using a data-driven approach instead of a rule-based approach, language independence is ensured. In comparison to the \ac{MIRA} algorithm, the word-level accuracy of the \ac{DNN}-based approach (two hidden layers) is only 0.78\,\% worse but enables a reduction of the model size by a factor of more than 250 to 36.7\,KB.
The second task, the phonetic transcription, is the process of translating letters to phonemes (also referred to as letter-to-sound or grapheme-to-phoneme) and is also chosen to be implemented in a data-driven approach, due to the same reason as in the syllabification. By using a \ac{DNN} with two hidden layers, a word-level accuracy of 96.16\,\% can be achieved, which is a decrease of only 0.13\,\% compared to the \ac{MIRA}-based technique. However, the model size again is reduced significantly from around 1.4\,MB to 43.4\,KB.
To determine how a word is spoken, its part-of-speech tagging (the third task) is one of the basic features. One important aspect of allocating a uniquely interpretable tag to each word in a sentence is to identify the correct pronunciation of a homograph\footnote{A word with different meanings depending on the pronunciation}. In this part, two \acp{DNN} were deployed (both with two hidden layers), one to replace the lexicon containing the mapping of words to their syntactic information and one which performs the tagging instead of a standard feed-forward \ac{NN}. In this task, the biggest reduction of the model size could be achieved, from almost 100\,MB to about 178\,KB. Thereby, the word-level accuracy got reduced by 3.03\,\%. This reduction is due to the lack of fine-tuning of the \acp{DNN}, which in this scope has not been conducted.
The last task is the lexical stress prediction, which represents an essential part of prosody generation, by indicating which syllables have to be given a special emphasis during the utterance. The \ac{DNN} used for this task is equipped with three hidden layers and results in an overall accuracy of 97.67\,\% and a reduction of the model size by a factor of 60 to around 110\,KB.

\begin{table}[h]
	\caption{Resulting accuracy and footprint size~\cite{boros:robust}}	
	\vspace{-0.75em}
	\label{tab:deepresults}%
	\raggedright
	\resizebox{0.74\columnwidth}{!}{
		\begin{minipage}{\columnwidth}
			\centering	
			\begin{tabularx}{1.35\columnwidth}{*{3}{l>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X}}
				\toprule
				& \multicolumn{2}{c}{\textbf{SYL}} & \multicolumn{2}{c}{\textbf{PT}} & \multicolumn{2}{c}{\textbf{POT}} & \multicolumn{2}{c}{\textbf{LSP}}\\[0.25em]
				& MIRA & DNN & MIRA & DNN & NN & DNN & MIRA & DNN\\
				\midrule
				Accuracy & 99.01\,\% & 98.23\,\% & 96.29\,\% & 96.16\,\% & 98.19\,\% & 95.16\,\% & 98.80\,\% & 97.67\,\%\\[0.5em]
				Size & 9.4\,MB & 36.7\,KB & 1.4\,MB & 43.4\,KB & 96\,MB & 178\,KB & 6\,MB & 110\,KB\\
				\bottomrule
			\end{tabularx}
		\end{minipage}}
	%\vspace{-0.75em}
\end{table}

\noindent Overall, it can be concluded that the measures introduced in~\cite{boros:robust} enabled a tremendous reduction of the model sizes, while keeping the performance almost as high as in the previous models. From these results, it can be deduced that the introduction of deep learning models like \acp{DNN} into the front-end or text processing part of a \ac{TTS}-system is a highly reasonable strategy to port speech synthesis systems onto mobile devices without relying anymore on Internet access.
%\vspace{2em}