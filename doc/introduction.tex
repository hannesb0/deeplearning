%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%																				%%
%% File name: 		introduction.tex											%%
%% Project name:	Applications in Deep Learning								%%
%% Type of work:	Advanced Seminar											%%
%% Author:			Hannes Bohnengel											%%
%% Mentor:			Debayan Roy													%%
%% Date:			22 May 2017													%%
%% University:		Technical University of Munich								%%
%% Comments:		Created in texstudio with tab width = 4						%%
%%																				%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}



\begin{enumerate}
	\item Why speech synthesis is important? What are its applications?
	\item What are the conventional techniques of speech synthesis? What are the drawbacks of such techniques?
	\item What is deep learning? What improvements do deep learning algorithms bring?
	\item How some algorithms are modified to suit speech synthesis?
	\item Why is it important to implement speech synthesis on embedded platform?
	\item An example of how speech synthesis can be implemented on embedded platform without deep learning.
	\item How the 3 can be combined?
	\item Future works.
\end{enumerate}


These are the core papers: 
\begin{itemize} %%{label}{spacing}
	\item Robust Deep-learning Models for Text-to-speech Synthesis Support on Embedded Devices \cite{boros:robust}
	\item Statistical parametric speech synthesis using deep neural networks \cite{ze:statistical}
	\item Deep neural networks employing Multi-Task Learning and stacked bottleneck features for speech synthesis \cite{wu:deep}
	\item Efficient deep neural networks for speech synthesis using bottleneck features \cite{joo:efficient}
	\item On the training aspects of Deep Neural Network (DNN) for parametric TTS synthesis \cite{qian:training}
	\item TTS synthesis with bidirectional LSTM based recurrent neural networks \cite{fan:tts}
	\item The effect of neural networks in statistical parametric speech synthesis \cite{hashimoto:effect}
	\item Efficient memory compression in deep neural networks using coarse-grain sparsification for speech applications \cite{kadetotad:efficient}
	\item Speeding up deep neural networks for speech recognition on ARM Cortex-A series processors \cite{xing:speeding}
\end{itemize}

These are interesting references:

\begin{itemize} %%{label}{spacing}
	\item Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends \cite{ling:deep}
\end{itemize}

\newpage